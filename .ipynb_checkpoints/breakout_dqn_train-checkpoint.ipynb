{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16cea49a-5025-4777-bc8f-21b8e668b696",
   "metadata": {},
   "source": [
    "# Train Breakout Game on DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1b9141-b8b7-4792-894c-bebfa9819cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# !{os.sys.executable} -m pip install gymnasium\n",
    "# !{os.sys.executable} -m pip install cmake \n",
    "# !{os.sys.executable} -m pip install ale-py\n",
    "# !{os.sys.executable} -m pip install atari-py\n",
    "# !{os.sys.executable} -m pip install Pillow\n",
    "# !{os.sys.executable} -m pip install ipython\n",
    "# !{os.sys.executable} -m pip install pygame\n",
    "# !{os.sys.executable} -m pip install torchsummary\n",
    "# !{os.sys.executable} -m pip install tensorboardX\n",
    "# !{os.sys.executable} -m pip install \"gymnasium[atari, accept-rom-licesnse]\"\n",
    "# !{os.sys.executable} -m pip install \"autorom[accept-rom-license]\"\n",
    "# !{os.sys.executable} -m pip install torch\n",
    "# !{os.sys.executable} -m pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd0710a-2898-40b3-8892-eeed7f66aeb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import useful packages\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as Fv\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from DQN_Model import DQN\n",
    "from DQN_Model_improved import DQN_improved\n",
    "from DQN_Model_CNN import DQN_CNN\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from collections import namedtuple, deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1691deb0-fb41-422b-b9d1-4f43db2ce9c5",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b3643a0-f7e7-4ee4-b060-2b04ef761507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env_name = \"ALE/Breakout-v5\" # atari\n",
    "# env_name = \"ALE/Pong-v5\"\n",
    "# env_name = \"Breakout-v0\"\n",
    "# env_name = \"MountainCar-v0\" # classic control \n",
    "# env_name = \"Pendulum-v1\" # Classic control games, pendulum or pole cart... \n",
    "\n",
    "CNN_mode = True # False for mlp, True for CNN\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 32 # 32\n",
    "lr = 0.00001 #1lr = 0.0001\n",
    "initial_exploration = 10000 #1000\n",
    "goal_score = 200\n",
    "log_interval = 1 # 10\n",
    "update_target = 25 # 100\n",
    "replay_memory_capacity = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "env=gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36412c-122c-4abd-a1b0-b71025f13ec5",
   "metadata": {},
   "source": [
    "## Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a86295a-038c-491a-9bb9-c549ccfc2f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'next_state', 'action', 'reward', 'mask')\n",
    ")\n",
    "Transition_Biased = namedtuple(\n",
    "    'Transition', ('state', 'next_state', 'action', 'reward', 'mask', 'weight')\n",
    ")\n",
    "\n",
    "class Memory_DQN(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, next_state, action, reward, mask):\n",
    "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class Memory_DQN_biased(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        self.weights = np.ones(capacity)\n",
    "        self.idx = 0\n",
    "\n",
    "    def push(self, state, next_state, action, reward, mask):\n",
    "        weight = 1.0  # Initial weight for the new sample\n",
    "        self.memory.append(Transition_Biased(state, next_state, action, reward, mask, weight))\n",
    "        self.weights[self.idx] = weight\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        weights = self.weights / np.sum(self.weights)  # Normalize weights\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=weights)\n",
    "        batch = [self.memory[i] for i in indices]\n",
    "        \n",
    "        # Update weights based on the chosen samples\n",
    "        self.weights[indices] = [self.memory[i].weight for i in indices]\n",
    "        \n",
    "        batch = Transition_Biased(*zip(*batch))\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15352ad3-d6d6-4abb-b184-e19168c51b88",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c79a050-ba5a-4de5-b8f3-9a79ecac021c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_action(state, target_net, epsilon, env):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample() #random action\n",
    "    else:\n",
    "        return target_net.get_action(state)\n",
    "    \n",
    "# Define a transformation to convert RGB images to grayscale and resize them\n",
    "preprocess = transforms.Compose([\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    # transforms.Resize((210, 160)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_state(pre_state, CNN_mode = False):\n",
    "    # Convert the image to a PyTorch tensor and apply the defined transformation\n",
    "    # post_state = preprocess(pre_state)\n",
    "    # post_state = processed_image.unsqueeze(0) # Add a channel dimension\n",
    "    post_state = pre_state\n",
    "    if not CNN_mode:\n",
    "        post_state = torch.flatten(post_state) # reduces to flatten for mlp\n",
    "    return post_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a8486-e850-4b49-a870-efba0a2a1847",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8643f7-984a-48ec-987e-d5cfa532326e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_size 100800\n",
      "action_size 4\n",
      "state_shape (210, 160, 3)\n",
      "Broken\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "state_size = env.observation_space.low.size\n",
    "# state_size = int(state_size / 3) # remove the color from the game. We don't need it\n",
    "    \n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(\"state_size\", state_size)\n",
    "print(\"action_size\", action_size)\n",
    "\n",
    "state_shape = env.observation_space.low.shape\n",
    "if CNN_mode:\n",
    "    state_shape = (state_shape[0], state_shape[1], 3) # remove color from the game\n",
    "    cnn_shape = (int(3), int(state_shape[0]), int(state_shape[1]))\n",
    "    print(\"state_shape\", state_shape)\n",
    "\n",
    "if not CNN_mode: # False for mlp, True for CNN\n",
    "    online_net = DQN(state_size, action_size).to(device)\n",
    "    target_net = DQN(state_size, action_size).to(device)\n",
    "    # online_net = DQN_improved(state_size, action_size).to(device)\n",
    "    # target_net = DQN_improved(state_size, action_size).to(device)\n",
    "else:\n",
    "    online_net = DQN_CNN(state_shape, action_size).to(device)\n",
    "    target_net = DQN_CNN(state_shape, action_size).to(device)\n",
    "\n",
    "\n",
    "online_net.train()\n",
    "target_net.train()\n",
    "writer = SummaryWriter('logs')\n",
    "\n",
    "if not CNN_mode:\n",
    "    summary(online_net, input_size = (batch_size, state_size)) \n",
    "else:\n",
    "    # summary(online_net, input_size = (batch_size, cnn_shape))\n",
    "    print(\"Broken\")\n",
    "    \n",
    "optimizer = optim.Adam(online_net.parameters(), lr=lr)\n",
    "N_EPISODES = 1000 # 5000\n",
    "\n",
    "#initialize running variables\n",
    "running_score = 0\n",
    "epsilon = 1.0\n",
    "epsilon_decay_rate = 0.00005 #0.000005\n",
    "steps = 0\n",
    "loss = 0\n",
    "\n",
    "# initialize the memory bank\n",
    "# memory = Memory_DQN(replay_memory_capacity)\n",
    "memory = Memory_DQN_biased(replay_memory_capacity)\n",
    "\n",
    "# Before training\n",
    "loss_record = []\n",
    "scores_record = []\n",
    "best_score = float('-inf')  # Initialize best_score with negative infinity\n",
    "best_online_net_weights = None\n",
    "best_target_net_weights = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8965e0de-6afc-4db8-8dd8-c92c8e2420a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 14 (2518368411.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    if isinstance(action, np.ndarray):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 14\n"
     ]
    }
   ],
   "source": [
    "for episode in range(N_EPISODES):\n",
    "    done = False\n",
    "\n",
    "    score = 0\n",
    "    state = env.reset()[0]\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    state = preprocess_state(state, CNN_mode)\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        action = get_action(state, target_net, epsilon, env)\n",
    "        \n",
    "        # next_state, reward, done, _, _ = env.step(action) # 5 outputs\n",
    "        if isinstance(action, np.ndarray):\n",
    "            # print(\"fixed?\")\n",
    "            action = action[0]\n",
    "        next_state, reward, terminated, truncated,_ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        \n",
    "        next_state = torch.Tensor(next_state).to(device)\n",
    "        next_state = preprocess_state(next_state, CNN_mode)\n",
    "\n",
    "        mask = 0 if done else 1 #don't know what this line does\n",
    "\n",
    "        # i assume this is the penalty function?\n",
    "        reward = reward if not done or score == 499 else -1\n",
    "\n",
    "        action_one_hot = np.zeros(action_size)\n",
    "        action_one_hot[action] = 1\n",
    "\n",
    "        # add to memory bank\n",
    "        memory.push(state, next_state, action_one_hot, reward, mask)\n",
    "\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if steps > initial_exploration:\n",
    "            epsilon -= epsilon_decay_rate\n",
    "            epsilon = max(epsilon, 0.01)\n",
    "\n",
    "            # process the batch\n",
    "            batch = memory.sample(batch_size)\n",
    "            # print(\"batch\", batch)\n",
    "            states = torch.stack(batch.state).to(device)\n",
    "            next_states = torch.stack(batch.next_state).to(device)\n",
    "            actions = torch.Tensor(batch.action).float().to(device)\n",
    "            rewards = torch.Tensor(batch.reward).to(device)\n",
    "            masks = torch.Tensor(batch.mask).to(device)\n",
    "            \n",
    "            # print(\"states\", states.shape)\n",
    "            \n",
    "            q_values = online_net(states)\n",
    "            # print(\"q_values\", q_values.shape)\n",
    "            q_values = q_values.gather(1, actions.long())\n",
    "            next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "\n",
    "            q_target  = rewards + gamma * next_q_values*masks\n",
    "            \n",
    "\n",
    "            \n",
    "            loss = F.mse_loss(q_values, q_target.unsqueeze(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if steps % initial_exploration:\n",
    "                target_net.load_state_dict(online_net.state_dict())\n",
    "    score = score if score == 500.0 else score + 1\n",
    "    if running_score == 0:\n",
    "        running_score = score\n",
    "    else:\n",
    "        running_score = 0.99 * running_score + 0.01 * score\n",
    "\n",
    "    scores_record.append(running_score)\n",
    "    loss_record.append(loss)\n",
    "\n",
    "     # Update best_score and store best model weights in memory\n",
    "    if running_score > best_score:\n",
    "        best_score = running_score\n",
    "        best_online_net_weights = online_net.state_dict()\n",
    "        best_target_net_weights = target_net.state_dict()\n",
    "    \n",
    "    if episode % log_interval == 0:\n",
    "        print('\\r{} episode | score: {:.2f} | loss: {:.5f} | epsilon: {:.2f}'.format(\n",
    "            episode, running_score, loss, epsilon), end='')\n",
    "        writer.add_scalar('log/score', float(running_score), episode) # i don't know what this does\n",
    "        writer.add_scalar('log/loss', float(loss), episode)    \n",
    "\n",
    "    if running_score > goal_score:\n",
    "        break\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea523c5-cc44-48eb-b1a3-414c8b4edc88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create the saved_models folder if it doesn't exist\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Generate a unique train_number based on current timestamp\n",
    "train_number = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_folder = f'saved_models/train_{train_number}'\n",
    "\n",
    "# Create the train_number subfolder inside saved_models\n",
    "if not os.path.exists(train_folder):\n",
    "    os.makedirs(train_folder)\n",
    "\n",
    "# Save the model to the train_number subfolder\n",
    "torch.save(online_net.state_dict(), f'{train_folder}/online_net.pth')\n",
    "torch.save(target_net.state_dict(), f'{train_folder}/target_net.pth')\n",
    "\n",
    "# Save the best model weights to the train_number subfolder\n",
    "torch.save(best_online_net_weights, f'{train_folder}/best_online_net.pth')\n",
    "torch.save(best_target_net_weights, f'{train_folder}/best_target_net.pth')\n",
    "\n",
    "def convert_to_ints(lst):\n",
    "    return [int(elem.item()) if torch.is_tensor(elem) else int(elem) for elem in lst]\n",
    "\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot Rewards\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(scores_record)\n",
    "plt.title('Episode Scores')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Scores')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Losses (assuming you have a list of losses)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(convert_to_ints(loss_record))\n",
    "plt.title('Episode Losses')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plots to the train_number subfolder\n",
    "plt.savefig(f'{train_folder}/metrics_plot.png')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60198e2d-63ef-4e38-aac8-1fb05f13df99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
