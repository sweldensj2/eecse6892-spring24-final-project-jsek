{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16cea49a-5025-4777-bc8f-21b8e668b696",
   "metadata": {},
   "source": [
    "# Train Breakout Game on DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1b9141-b8b7-4792-894c-bebfa9819cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{os.sys.executable} -m pip install gymnasium\n",
    "# !{os.sys.executable} -m pip install cmake \n",
    "# !{os.sys.executable} -m pip install ale-py\n",
    "# !{os.sys.executable} -m pip install atari-py\n",
    "# !{os.sys.executable} -m pip install Pillow\n",
    "# !{os.sys.executable} -m pip install ipython\n",
    "# !{os.sys.executable} -m pip install pygame\n",
    "# !{os.sys.executable} -m pip install torchsummary\n",
    "# !{os.sys.executable} -m pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd0710a-2898-40b3-8892-eeed7f66aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful packages\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from DQN_Model import DQN\n",
    "from DQN_Model_improved import DQN_improved\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1691deb0-fb41-422b-b9d1-4f43db2ce9c5",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3643a0-f7e7-4ee4-b060-2b04ef761507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"Breakout-v0\"\n",
    "# env_name = \"MountainCar-v0\"\n",
    "env_name = \"Pendulum-v1\"\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "lr = 0.0001 #1lr = 0.0001\n",
    "initial_exploration = 10000 #1000\n",
    "goal_score = 200\n",
    "log_interval = 1 # 10\n",
    "update_target = 100\n",
    "replay_memory_capacity = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env=gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36412c-122c-4abd-a1b0-b71025f13ec5",
   "metadata": {},
   "source": [
    "## Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a86295a-038c-491a-9bb9-c549ccfc2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'next_state', 'action', 'reward', 'mask')\n",
    ")\n",
    "Transition_Biased = namedtuple(\n",
    "    'Transition', ('state', 'next_state', 'action', 'reward', 'mask', 'weight')\n",
    ")\n",
    "\n",
    "class Memory_DQN(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, next_state, action, reward, mask):\n",
    "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class Memory_DQN_biased(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        self.weights = np.ones(capacity)\n",
    "        self.idx = 0\n",
    "\n",
    "    def push(self, state, next_state, action, reward, mask):\n",
    "        weight = 1.0  # Initial weight for the new sample\n",
    "        self.memory.append(Transition_Biased(state, next_state, action, reward, mask, weight))\n",
    "        self.weights[self.idx] = weight\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        weights = self.weights / np.sum(self.weights)  # Normalize weights\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=weights)\n",
    "        batch = [self.memory[i] for i in indices]\n",
    "        \n",
    "        # Update weights based on the chosen samples\n",
    "        self.weights[indices] = [self.memory[i].weight for i in indices]\n",
    "        \n",
    "        batch = Transition_Biased(*zip(*batch))\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15352ad3-d6d6-4abb-b184-e19168c51b88",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c79a050-ba5a-4de5-b8f3-9a79ecac021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, target_net, epsilon, env):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample() #random action\n",
    "    else:\n",
    "        return target_net.get_action(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a8486-e850-4b49-a870-efba0a2a1847",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8643f7-984a-48ec-987e-d5cfa532326e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_size 3\n",
      "action_size 1\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 32, 128]             512\n",
      "            Linear-2                [-1, 32, 1]             129\n",
      "================================================================\n",
      "Total params: 641\n",
      "Trainable params: 641\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "state_size = env.observation_space.low.size\n",
    "# action_size = env.action_space.n\n",
    "action_size = 1\n",
    "\n",
    "print(\"state_size\", state_size)\n",
    "print(\"action_size\", action_size)\n",
    "\n",
    "online_net = DQN(state_size, action_size).to(device)\n",
    "target_net = DQN(state_size, action_size).to(device)\n",
    "# online_net = DQN_improved(state_size, action_size).to(device)\n",
    "# target_net = DQN_improved(state_size, action_size).to(device)\n",
    "\n",
    "\n",
    "online_net.train()\n",
    "target_net.train()\n",
    "writer = SummaryWriter('logs')\n",
    "\n",
    "summary(online_net, input_size = (batch_size, state_size)) \n",
    "\n",
    "optimizer = optim.Adam(online_net.parameters(), lr=lr)\n",
    "N_EPISODES = 10000 # 5000\n",
    "\n",
    "#initialize running variables\n",
    "running_score = 0\n",
    "epsilon = 1.0\n",
    "epsilon_decay_rate = 0.000005 #0.000005\n",
    "steps = 0\n",
    "loss = 0\n",
    "\n",
    "# initialize the memory bank\n",
    "# memory = Memory_DQN(replay_memory_capacity)\n",
    "memory = Memory_DQN_biased(replay_memory_capacity)\n",
    "\n",
    "# Before training\n",
    "loss_record = []\n",
    "scores_record = []\n",
    "best_score = float('-inf')  # Initialize best_score with negative infinity\n",
    "best_online_net_weights = None\n",
    "best_target_net_weights = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8965e0de-6afc-4db8-8dd8-c92c8e2420a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/hfpwj42x3zd2qyh387_9zcvm0000gn/T/ipykernel_94757/2015699533.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  action_one_hot[0] = action\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 episode | score: -1182.65 | loss: 0.00000 | epsilon: 1.00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/hfpwj42x3zd2qyh387_9zcvm0000gn/T/ipykernel_94757/2015699533.py:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  actions = torch.Tensor(batch.action).float().to(device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(batch\u001b[38;5;241m.\u001b[39mreward)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(batch\u001b[38;5;241m.\u001b[39mmask)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 47\u001b[0m q_values \u001b[38;5;241m=\u001b[39m online_net(states)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     48\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m target_net(next_states)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     50\u001b[0m q_target  \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_q_values\u001b[38;5;241m*\u001b[39mmasks\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index -1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "for episode in range(N_EPISODES):\n",
    "    done = False\n",
    "\n",
    "    score = 0\n",
    "    state = env.reset()[0]\n",
    "    state = torch.Tensor(state).to(device)\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        action = get_action(state, target_net, epsilon, env)\n",
    "        \n",
    "        # next_state, reward, done, _, _ = env.step(action) # 5 outputs\n",
    "        next_state, reward, terminated, truncated,_ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        \n",
    "        next_state = torch.Tensor(next_state).to(device)\n",
    "\n",
    "        mask = 0 if done else 1 #don't know what this line does\n",
    "\n",
    "        # i assume this is the penalty function?\n",
    "        reward = reward if not done or score == 499 else -1\n",
    "\n",
    "        action_one_hot = np.zeros(action_size)\n",
    "        # action_one_hot[action] = 1\n",
    "        action_one_hot[0] = action\n",
    "\n",
    "        # add to memory bank\n",
    "        memory.push(state, next_state, action_one_hot, reward, mask)\n",
    "\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if steps > initial_exploration:\n",
    "            epsilon -= epsilon_decay_rate\n",
    "            epsilon = max(epsilon, 0.01)\n",
    "\n",
    "            # process the batch\n",
    "            batch = memory.sample(batch_size)\n",
    "            # print(\"batch\", batch)\n",
    "            states = torch.stack(batch.state).to(device)\n",
    "            next_states = torch.stack(batch.next_state).to(device)\n",
    "            actions = torch.Tensor(batch.action).float().to(device)\n",
    "            rewards = torch.Tensor(batch.reward).to(device)\n",
    "            masks = torch.Tensor(batch.mask).to(device)\n",
    "\n",
    "            q_values = online_net(states).gather(1, actions.long())\n",
    "            next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "\n",
    "            q_target  = rewards + gamma * next_q_values*masks\n",
    "            \n",
    "\n",
    "            \n",
    "            loss = F.mse_loss(q_values, q_target.unsqueeze(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if steps % initial_exploration:\n",
    "                target_net.load_state_dict(online_net.state_dict())\n",
    "    score = score if score == 500.0 else score + 1\n",
    "    if running_score == 0:\n",
    "        running_score = score\n",
    "    else:\n",
    "        running_score = 0.99 * running_score + 0.01 * score\n",
    "\n",
    "    scores_record.append(running_score)\n",
    "    loss_record.append(loss)\n",
    "\n",
    "     # Update best_score and store best model weights in memory\n",
    "    if running_score > best_score:\n",
    "        best_score = running_score\n",
    "        best_online_net_weights = online_net.state_dict()\n",
    "        best_target_net_weights = target_net.state_dict()\n",
    "    \n",
    "    if episode % log_interval == 0:\n",
    "        print('\\r{} episode | score: {:.2f} | loss: {:.5f} | epsilon: {:.2f}'.format(\n",
    "            episode, running_score, loss, epsilon), end='')\n",
    "        writer.add_scalar('log/score', float(running_score), episode) # i don't know what this does\n",
    "        writer.add_scalar('log/loss', float(loss), episode)    \n",
    "\n",
    "    if running_score > goal_score:\n",
    "        break\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea523c5-cc44-48eb-b1a3-414c8b4edc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create the saved_models folder if it doesn't exist\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Generate a unique train_number based on current timestamp\n",
    "train_number = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_folder = f'saved_models/train_{train_number}'\n",
    "\n",
    "# Create the train_number subfolder inside saved_models\n",
    "if not os.path.exists(train_folder):\n",
    "    os.makedirs(train_folder)\n",
    "\n",
    "# Save the model to the train_number subfolder\n",
    "torch.save(online_net.state_dict(), f'{train_folder}/online_net.pth')\n",
    "torch.save(target_net.state_dict(), f'{train_folder}/target_net.pth')\n",
    "\n",
    "# Save the best model weights to the train_number subfolder\n",
    "torch.save(best_online_net_weights, f'{train_folder}/best_online_net.pth')\n",
    "torch.save(best_target_net_weights, f'{train_folder}/best_target_net.pth')\n",
    "\n",
    "def convert_to_ints(lst):\n",
    "    return [int(elem.item()) if torch.is_tensor(elem) else int(elem) for elem in lst]\n",
    "\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot Rewards\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(scores_record)\n",
    "plt.title('Episode Scores')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Scores')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Losses (assuming you have a list of losses)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(convert_to_ints(loss_record))\n",
    "plt.title('Episode Losses')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plots to the train_number subfolder\n",
    "plt.savefig(f'{train_folder}/metrics_plot.png')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
